{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hand classification\n",
        "\n",
        "This notebook allows us to work with neural network-based hand classification models (*using Tensorflow 2*) with a more **user-friendly interface**, without the need to make any changes to the code. The task consists of given a hand, predicting the gender of the individual.\n",
        "\n",
        "Although the GPU can speed up the inference, we can work with the CPU without problems, as the inference time for an image is very low even on the CPU (a few microseconds).\n",
        "\n",
        "    Author: @AAitorG"
      ],
      "metadata": {
        "id": "PcLCxMoW5SjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  { display-mode: \"form\" }\n",
        "#@markdown <h3> Utilities + Download & Load model (run this!) </h3>\n",
        "\n",
        "!pip install tensorflow==2.12 keras==2.12\n",
        "\n",
        "from PIL import Image\n",
        "from skimage.measure import label, regionprops\n",
        "import numpy as np\n",
        "\n",
        "def get_just_hand(np_img, image_size=256):\n",
        "\n",
        "    props = regionprops(np_img) # (min_row, min_col, max_row, max_col) -- [min; max)\n",
        "    min_row, min_col, max_row, max_col = props[0]['bbox']\n",
        "    hand_cols = max_col - min_col\n",
        "    remain = hand_cols - image_size\n",
        "    left = True\n",
        "    N_col = np_img.shape[1]\n",
        "\n",
        "    if remain < 0:\n",
        "        # hand is smaller so, we need to add extra padding\n",
        "        remain = abs(remain) # now just count remaining columns\n",
        "        for i in range(remain):\n",
        "            if left:\n",
        "                if min_col > 0: min_col -= 1\n",
        "                elif max_col+1 < N_col: max_col += 1\n",
        "                else: print('ERROR: image is not big enough')\n",
        "            else:\n",
        "                if max_col+1 < N_col: max_col += 1\n",
        "                elif min_col > 0: min_col -= 1\n",
        "                else: print('ERROR')\n",
        "            left = not left\n",
        "    elif remain > 0:\n",
        "        # hand is bigger, so we need to cut a bit more\n",
        "        for i in range(remain):\n",
        "            if left: min_col += 1\n",
        "            else: max_col -= 1\n",
        "            left = not left\n",
        "\n",
        "    return np_img[:, min_col:max_col]\n",
        "\n",
        "def load_and_preprocess_image(img_path, rotate_180=False, white_hand=True, output_img_size=256):\n",
        "    # The image is the binary mask\n",
        "\n",
        "    image = Image.open(img_path)\n",
        "    # Convert (RGBA,...) images to RGB\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    width, height = image.size\n",
        "    shorter_h = height <= width\n",
        "    factor = output_img_size/height if shorter_h else output_img_size/width\n",
        "    new_width, new_height = round(width*factor), round(height*factor)\n",
        "    image = image.resize((new_width, new_height), Image.NEAREST) # resize equally height and width until shortest side is 256px long\n",
        "\n",
        "    if not shorter_h:\n",
        "        image = image.transpose(Image.ROTATE_90)\n",
        "\n",
        "    # labeled img - cropp by bbox\n",
        "    if rotate_180:\n",
        "        image = image.transpose(Image.ROTATE_180) # rock\n",
        "    np_img = (np.array(image)[:,:,0] > 128).astype(np.uint8)\n",
        "    if not white_hand:\n",
        "        np_img = 1-np_img\n",
        "\n",
        "    if height != width:\n",
        "        np_img = get_just_hand(np_img, output_img_size)\n",
        "\n",
        "    np_img = np_img * 255\n",
        "    np_img = np.expand_dims(np_img, axis=-1)\n",
        "    np_img = np.concatenate([np_img, np_img, np_img], axis=-1)\n",
        "    gt_img = Image.fromarray(np_img.astype(np.uint8))\n",
        "\n",
        "    if not shorter_h:\n",
        "        gt_img = gt_img.transpose(Image.ROTATE_270)\n",
        "    return np.array(gt_img)\n",
        "\n",
        "# UPLOAD\n",
        "from google.colab import files\n",
        "#print(\"Upload model (.h5 format):\")\n",
        "#uploaded = files.upload()\n",
        "#model_name = list(uploaded.keys())[0]\n",
        "#print('\\nmodel_name:', model_name)\n",
        "\n",
        "# DOWNLOAD\n",
        "import os\n",
        "model_name = './hand-classifier-weights-1neuron-effnetv2S.h5'\n",
        "if not os.path.exists(model_name):\n",
        "    !wget 'https://www.dropbox.com/s/5jmc2a0bp19tu7c/hand-classifier-weights-1neuron-effnetv2S.h5?dl=0' -O './hand-classifier-weights-1neuron-effnetv2S.h5'\n",
        "\n",
        "# LOAD model\n",
        "from tensorflow.keras.models import load_model\n",
        "try:\n",
        "    if model != None:\n",
        "        del model\n",
        "except:\n",
        "    pass\n",
        "model = load_model(model_name)\n",
        "print('Model loaded')"
      ],
      "metadata": {
        "id": "k1NpcXvi2Juj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf3a0ec-110f-4428-d6d2-4655e686ca10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-14 11:29:29--  https://www.dropbox.com/s/5jmc2a0bp19tu7c/hand-classifier-weights-1neuron-effnetv2S.h5?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601f:18::a27d:912\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/5jmc2a0bp19tu7c/hand-classifier-weights-1neuron-effnetv2S.h5 [following]\n",
            "--2023-03-14 11:29:29--  https://www.dropbox.com/s/raw/5jmc2a0bp19tu7c/hand-classifier-weights-1neuron-effnetv2S.h5\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb3d20e18dc6043a47dcaefb432.dl.dropboxusercontent.com/cd/0/inline/B4OwY2Y2WeJWrBAshZRHy6biWog_r8EAnNxmhO7e5kEIYPf8Oc7a6REEFv9ZZJwE1qIpWxpZpfuvni7TErYw7LPgu5u09ImV5NoBsQS-KxryMPrm0wbVdMZzxtCcnGqUmjauoZZQO2n1zF1Cvjpa0FIHglmsmjb5j9WkIPc6yQnGcA/file# [following]\n",
            "--2023-03-14 11:29:29--  https://ucb3d20e18dc6043a47dcaefb432.dl.dropboxusercontent.com/cd/0/inline/B4OwY2Y2WeJWrBAshZRHy6biWog_r8EAnNxmhO7e5kEIYPf8Oc7a6REEFv9ZZJwE1qIpWxpZpfuvni7TErYw7LPgu5u09ImV5NoBsQS-KxryMPrm0wbVdMZzxtCcnGqUmjauoZZQO2n1zF1Cvjpa0FIHglmsmjb5j9WkIPc6yQnGcA/file\n",
            "Resolving ucb3d20e18dc6043a47dcaefb432.dl.dropboxusercontent.com (ucb3d20e18dc6043a47dcaefb432.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucb3d20e18dc6043a47dcaefb432.dl.dropboxusercontent.com (ucb3d20e18dc6043a47dcaefb432.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 244566688 (233M) [text/plain]\n",
            "Saving to: ‘./hand-classifier-weights-1neuron-effnetv2S.h5’\n",
            "\n",
            "./hand-classifier-w 100%[===================>] 233.24M  32.0MB/s    in 7.7s    \n",
            "\n",
            "2023-03-14 11:29:38 (30.2 MB/s) - ‘./hand-classifier-weights-1neuron-effnetv2S.h5’ saved [244566688/244566688]\n",
            "\n",
            "Model loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  { display-mode: \"form\" }\n",
        "#@markdown <h3> Upload image </h3>\n",
        "\n",
        "#@markdown *Note: You can run this cell as many times as you need.*\n",
        "print(\"Upload image:\")\n",
        "uploaded = files.upload()\n",
        "image_name = list(uploaded.keys())[0]\n",
        "print('Uploaded filename:', image_name)\n",
        "print(\"Uploaded image:\")\n",
        "Image.open(image_name)"
      ],
      "metadata": {
        "id": "zuMsV-68yZvQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "d6ddbd07-f6aa-4ecd-9698-042a94f24159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5ee5eab3-f196-41e9-91a9-77b234eb8abe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5ee5eab3-f196-41e9-91a9-77b234eb8abe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 0_Hand_0000086.png to 0_Hand_0000086.png\n",
            "Uploaded filename: 0_Hand_0000086.png\n",
            "Uploaded image:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=256x256 at 0x7F81247375B0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center> Preprocessing </h1>\n",
        "\n",
        "Preprocessing includes: centring the hand, resizing and cropping the image to 256x256.\n",
        "\n",
        "Before the image is given to the model, the image must look as shown in Fig. 1. In summary, the important features to consider are:\n",
        "\n",
        "1.   The image must include the semantic segmentation of a single hand.\n",
        "2.   The hand must be white and the background black.\n",
        "3.   The fingers must face downwards.\n",
        "\n",
        "The 2nd and 3rd point can be easily corrected by using the checkboxes in the following cell or form. The `_image_name` is the file name of the image we want to modify and use, if we leave this option empty, the last uploaded image will be used by default.\n",
        "\n",
        "Once the form is completed, we only have to run the cell and continue.\n",
        "\n",
        "___\n",
        "\n",
        "*Note: The `_image_name` file names are only valid if those images have been previously uploaded during the same session. Once the machine is switched off, previously uploaded data will be lost.*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<body>\n",
        "<center>\n",
        "<cropped>\n",
        "<img src=\"https://www.dropbox.com/s/fm9x4iiyigve2ux/Hand%20segmentation%20example.png?dl=1\" alt=\"Binary label image\" width=\"256\" >\n",
        "</cropped>\n",
        "<figcaption>(Fig. 1) Example of semantic segmentation of a hand. This example contains a female hand.</figcaption>\n",
        "</body>\n"
      ],
      "metadata": {
        "id": "K74eGd6-aZSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "#@markdown <h1> Loading and Pre-processing the image </h1>\n",
        "#@markdown Once the pre-processed image looks like the image in the given example (Fig. 1),\n",
        "#@markdown we can go ahead and make the prediction using the model.\n",
        "\n",
        "#@markdown ---\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sigmoid = lambda x : 1/(1 + np.exp(-x))\n",
        "label2tag = {0:\"Female\", 1:\"Male\"}\n",
        "\n",
        "#@markdown Check if the hand is black in the uploaded image.\n",
        "hand_is_black = False #@param {type:\"boolean\"}\n",
        "#@markdown Check whether the fingers are pointing upwards in the uploaded image.\n",
        "fingers_are_facing_upwards = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown (OPTIONAL) Write the filename of the image to be used.\n",
        "#@markdown\n",
        "#@markdown  * *By default: last uploaded image will be used.*\n",
        "_image_name = '' #@param {type:\"string\"}\n",
        "if _image_name != '': image_name = _image_name\n",
        "\n",
        "img = load_and_preprocess_image(image_name, rotate_180=fingers_are_facing_upwards,\n",
        "                                white_hand=not hand_is_black)\n",
        "print(\"Make sure the hand is white and the fingers are facing downwards in the following image:\")\n",
        "print('\\n', image_name)\n",
        "print('', img.shape)\n",
        "\n",
        "Image.fromarray(img)"
      ],
      "metadata": {
        "id": "PoPjHY8CyYk2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "596f16d8-0017-4cb6-901b-8ad5c936e023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Make sure the hand is white and the fingers are facing downwards in the following image:\n",
            "\n",
            " 0_Hand_0000086.png\n",
            " (256, 256, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x256 at 0x7F808BFBC160>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Predicted value` shows the output of the model after applying the\n",
        "sigmoidal function to it. The value has a $[0,1]$ range where $0$ means the\n",
        "predicted class is \"*Female*\" and $1$ means that the predicted class is \"*Male*\".\n",
        "\n",
        "With this in mind we will consider that the `Prediction`\n",
        "or predicted class will be \"*Male*\" if `Predicted value` > 0.5\n",
        "otherwise, the predicted class will be \"*Female*\".\n",
        "\n",
        "Finally, we will consider the level of `Confidence` of the predicted class as:\n",
        "\n",
        "$$ Confidence = \\left\\{ \\begin{array}{cl}\n",
        "Predicted Value & if \\ \\ \\ \\ Predicted Value > 0.5 \\\\\n",
        "1-Predicted Value & else\n",
        "\\end{array} \\right. $$\n",
        "\n",
        "The confidence level of each class has a range [0, 1], where $0$ means that there is no confidence, and $1$ means that there is very high confidence that it belongs to that class. Note that both probabilities are complementary\n",
        "\n",
        "$$p + (1-p) = 1$$\n",
        "\n",
        "where $p$ is the probability of belonging to the class \"*Male*\" and $(1-p)$ is the probability of belonging to the class \"*Female*\". Therefore, the predicted class will always have at least $0.5$ confidence."
      ],
      "metadata": {
        "id": "he_wQpcY1O9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  { display-mode: \"form\" }\n",
        "#@markdown <h3> Use the model and make a prediction </h3>\n",
        "prediction = model.predict_on_batch(np.array([img]))\n",
        "pred_val = sigmoid(prediction[0][0])\n",
        "pred_class = 1 if pred_val>0.5 else 0\n",
        "conf = pred_val if pred_val>0.5 else 1-pred_val\n",
        "\n",
        "print('Input:', image_name)\n",
        "Image.fromarray(img).show()\n",
        "print('\\nPredicted value:', round(pred_val, 5), '\\n' + '-'*30)\n",
        "print('Prediction:', label2tag[pred_class])\n",
        "print('Confidence:', round(conf, 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "BjYg2BgqXpau",
        "outputId": "96f7f5c0-5491-4e5b-f34c-0cc506a8d18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 0_Hand_0000086.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x256 at 0x7F808BFB6280>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted value: 0.20841 \n",
            "------------------------------\n",
            "Prediction: Female\n",
            "Confidence: 0.79159\n"
          ]
        }
      ]
    }
  ]
}